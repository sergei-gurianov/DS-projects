{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.utils.extmath import stable_cumsum\n",
    "from sklift.models import SoloModel, ClassTransformation, TwoModels\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients = pd.read_csv('../input/x5-retail-hero/clients.csv', index_col='client_id',parse_dates=['first_issue_date','first_redeem_date'])\n",
    "df_train = pd.read_csv('../input/x5-retail-hero/uplift_train.csv', index_col='client_id')\n",
    "df_test = pd.read_csv('../input/x5-retail-hero/uplift_test.csv', index_col='client_id')\n",
    "df_products = pd.read_csv('../input/x5-retail-hero/products.csv', index_col='product_id').reset_index()\n",
    "df_purchases = pd.read_csv('../input/x5-retail-hero/purchases.csv',parse_dates=['transaction_datetime'],index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge df_purchases and df_products\n",
    "df_purchases.drop(columns=['express_points_received','express_points_spent','trn_sum_from_red'],inplace=True)\n",
    "df_purchases = df_purchases.merge(df_products[['netto','is_alcohol','is_own_trademark','product_id']], left_on='product_id', right_on='product_id').set_index(df_purchases.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uplift metrics\n",
    "def uplift_score(prediction, treatment, target, rate=0.3):\n",
    "    order = np.argsort(-prediction)\n",
    "    treatment_n = int((treatment == 1).sum() * rate)\n",
    "    treatment_p = target[order][treatment[order] == 1][:treatment_n].mean()\n",
    "    control_n = int((treatment == 0).sum() * rate)\n",
    "    control_p = target[order][treatment[order] == 0][:control_n].mean()\n",
    "    score = treatment_p - control_p\n",
    "    return score\n",
    "\n",
    "def get_score(valid_uplift,indices_valid,concat_train):\n",
    "    valid_score = uplift_score(\n",
    "        valid_uplift,\n",
    "        treatment=concat_train.iloc[indices_valid].treatment_flg.values,\n",
    "        target=concat_train.iloc[indices_valid].target.values,\n",
    "    )\n",
    "    return valid_score\n",
    "\n",
    "def uplift_curve(valid_uplift,indices_valid,concat_train):\n",
    "    y_true = concat_train.iloc[indices_valid].target.values\n",
    "    treatment=concat_train.iloc[indices_valid].treatment_flg.values\n",
    "    uplift = valid_uplift\n",
    "    \n",
    "    y_true, uplift, treatment = np.array(y_true), np.array(uplift), np.array(treatment)\n",
    "    desc_score_indices = np.argsort(uplift, kind=\"mergesort\")[::-1]\n",
    "    y_true, uplift, treatment = y_true[desc_score_indices], uplift[desc_score_indices], treatment[desc_score_indices]\n",
    "\n",
    "    y_true_ctrl, y_true_trmnt = y_true.copy(), y_true.copy()\n",
    "\n",
    "    y_true_ctrl[treatment == 1] = 0\n",
    "    y_true_trmnt[treatment == 0] = 0\n",
    "\n",
    "    distinct_value_indices = np.where(np.diff(uplift))[0]\n",
    "    threshold_indices = np.r_[distinct_value_indices, uplift.size - 1]\n",
    "\n",
    "    num_trmnt = stable_cumsum(treatment)[threshold_indices]\n",
    "    y_trmnt = stable_cumsum(y_true_trmnt)[threshold_indices]\n",
    "\n",
    "    num_all = threshold_indices + 1\n",
    "\n",
    "    num_ctrl = num_all - num_trmnt\n",
    "    y_ctrl = stable_cumsum(y_true_ctrl)[threshold_indices]\n",
    "\n",
    "    curve_values = (np.divide(y_trmnt, num_trmnt, out=np.zeros_like(y_trmnt), where=num_trmnt != 0) -\\\n",
    "                    np.divide(y_ctrl, num_ctrl, out=np.zeros_like(y_ctrl), where=num_ctrl != 0)) * num_all\n",
    "\n",
    "    if num_all.size == 0 or curve_values[0] != 0 or num_all[0] != 0:\n",
    "        num_all = np.r_[0, num_all]\n",
    "        curve_values = np.r_[0, curve_values]\n",
    "\n",
    "    return num_all, curve_values\n",
    "\n",
    "def auс_uplift(valid_uplift,indices_valid,concat_train):\n",
    "    y_true = concat_train.iloc[indices_valid].target.values\n",
    "    treatment=concat_train.iloc[indices_valid].treatment_flg.values\n",
    "    uplift = valid_uplift\n",
    "    \n",
    "    return auc(*uplift_curve(valid_uplift,indices_valid,concat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby sum,count,mean,nunique\n",
    "df_purchases['trans_hour'] = df_purchases.transaction_datetime.dt.hour\n",
    "df_purchases['dayofweek'] = df_purchases.transaction_datetime.dt.dayofweek\n",
    "\n",
    "group_by_7weeks = df_purchases[df_purchases['transaction_datetime'] > '2019-02-01'].groupby(['client_id','transaction_id'])\n",
    "group_by_2weeks = df_purchases[df_purchases['transaction_datetime'] > '2019-03-03'].groupby(['client_id','transaction_id'])\n",
    "\n",
    "last_cols = ['regular_points_received','regular_points_spent', 'purchase_sum','store_id','dayofweek','trans_hour']\n",
    "all_hist = group_by_7weeks[last_cols].last()\n",
    "two_weeks = group_by_2weeks[last_cols].last()\n",
    "\n",
    "sum_cols = ['netto','is_alcohol','is_own_trademark']\n",
    "all_hist_sum = group_by_7weeks[sum_cols].sum()\n",
    "two_weeks_sum = group_by_2weeks[sum_cols].sum()\n",
    "\n",
    "mean_cols = ['product_quantity','netto','trn_sum_from_iss']\n",
    "all_hist_mean = group_by_7weeks[mean_cols].mean()\n",
    "two_weeks_mean = group_by_2weeks[mean_cols].mean()\n",
    "\n",
    "nu_cols = ['product_quantity','product_id']\n",
    "all_hist_nu = group_by_7weeks[nu_cols].nunique()\n",
    "two_weeks_nu = group_by_2weeks[nu_cols].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['regular_points_received','regular_points_spent', 'purchase_sum','dayofweek','trans_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  pd.concat([\n",
    "                    all_hist.groupby('client_id')['purchase_sum'].count(),\n",
    "                    two_weeks.groupby('client_id')['purchase_sum'].count(),\n",
    "                    all_hist.groupby('client_id')['dayofweek'].mean(),\n",
    "                    two_weeks.groupby('client_id')['dayofweek'].mean(),\n",
    "                    all_hist.groupby('client_id')['trans_hour'].mean(),\n",
    "                    two_weeks.groupby('client_id')['trans_hour'].mean(),\n",
    "                    all_hist.groupby('client_id')[['store_id']].nunique(),\n",
    "                    two_weeks.groupby('client_id')[['store_id']].nunique(),\n",
    "                    #mean\n",
    "                    all_hist_mean.groupby('client_id')['product_quantity'].mean(),\n",
    "                    two_weeks_mean.groupby('client_id')['product_quantity'].mean(),\n",
    "                    all_hist_mean.groupby('client_id')['netto'].mean(),\n",
    "                    two_weeks_mean.groupby('client_id')['netto'].mean(),\n",
    "                    all_hist_mean.groupby('client_id')['trn_sum_from_iss'].mean(),\n",
    "                    two_weeks_mean.groupby('client_id')['trn_sum_from_iss'].mean(),\n",
    "                    #nu\n",
    "                    all_hist_nu.groupby('client_id')['product_id'].mean(),\n",
    "                    two_weeks_nu.groupby('client_id')['product_id'].mean(),\n",
    "                    all_hist_nu.groupby('client_id')['product_id'].sum(),\n",
    "                    two_weeks_nu.groupby('client_id')['product_id'].sum(),\n",
    "                    all_hist_nu.groupby('client_id')['product_quantity'].mean(),\n",
    "                    two_weeks_nu.groupby('client_id')['product_quantity'].mean(),\n",
    "                    #sum\n",
    "                    all_hist_sum.groupby('client_id')['is_alcohol'].sum(),\n",
    "                    two_weeks_sum.groupby('client_id')['is_alcohol'].sum(),\n",
    "                    all_hist_sum.groupby('client_id')['is_own_trademark'].sum(),\n",
    "                    two_weeks_sum.groupby('client_id')['is_own_trademark'].sum(),\n",
    "                    #casual\n",
    "                    all_hist.groupby('client_id').sum(),\n",
    "                    two_weeks.groupby('client_id').sum()\n",
    "                      ],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns = ['total_count_purchase','two_weeks_count_purchase']+['total_mean_dayofweek','two_weeks_mean_dayofweek']+\\\n",
    "    ['total_mean_trans_hour','two_weeks_mean_trans_hour']+['total_count_store_id','two_weeks_count_store_id']+\\\n",
    "    ['total_mean_product_quantity','two_weeks_mean_product_quantity']+['total_mean_netto','two_weeks_mean_netto']+\\\n",
    "    ['total_mean_trn_sum_from_iss','two_weeks_mean_trn_sum_from_iss']+\\\n",
    "    ['total_nu_product_id','two_weeks_nu_product_id']+['total_sum_product_id','two_weeks_sum_product_id']+\\\n",
    "    ['total_nu_product_quantity','two_weeks_nu_product_quantity']+\\\n",
    "    ['total_sum_is_alcohol','two_weeks_sum_is_alcohol']+['total_sum_is_own_trademark','two_weeks_sum_is_own_trademark']+\\\n",
    "    list(c+\"_all\" for c in names)+list(c+\"_two_weeks\" for c in names)\n",
    "\n",
    "features.drop(columns=['dayofweek_two_weeks','dayofweek_all','trans_hour_two_weeks','trans_hour_all'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = pd.concat([df_train,df_clients,features],axis = 1,sort = True)\n",
    "merged_train = merged_train[~merged_train['target'].isnull()].copy()\n",
    "#features\n",
    "merged_train['diff_quantity'] = merged_train['total_mean_product_quantity']-merged_train['two_weeks_mean_product_quantity']\n",
    "merged_train['prop_alc'] = merged_train['total_sum_is_alcohol']/merged_train['total_sum_product_id']\n",
    "merged_train['diff_total_points'] = merged_train['regular_points_received_all']-merged_train['regular_points_spent_all']\n",
    "merged_train['diff_two_weeks_points'] = merged_train['regular_points_received_two_weeks']-merged_train['regular_points_spent_two_weeks']\n",
    "merged_train['first_issue_date'] = merged_train['first_issue_date'].astype(int)/10**9\n",
    "merged_train['first_redeem_date'] = merged_train['first_redeem_date'].astype(int)/10**9\n",
    "merged_train['diff_time'] = merged_train['first_redeem_date']-merged_train['first_issue_date']\n",
    "merged_train['gender'] = list(ord(v[0]) for v in merged_train['gender'].values)\n",
    "#category features\n",
    "merged_train = merged_train.fillna(0)\n",
    "for col in ['total_mean_trans_hour','two_weeks_mean_trans_hour','total_mean_dayofweek','two_weeks_mean_dayofweek']:\n",
    "    merged_train[col] = round(merged_train.total_mean_dayofweek).astype('int')\n",
    "    merged_train[col] = merged_train[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill wrong age\n",
    "age_train = merged_train.loc[(merged_train.age>14) & (merged_train.age<100)]\n",
    "age_test = merged_train.loc[merged_train.age>100]\n",
    "\n",
    "age_params = {'learning_rate':0.01,'max_depth':6,'num_leaves':20,\n",
    "             'min_data_in_leaf':30, 'application':'binary',\n",
    "             'subsample':1, 'colsample_bytree': 0.8,\n",
    "             'reg_alpha':0.01,'data_random_seed':42,'metric':'binary_logloss'        \n",
    "                }\n",
    "\n",
    "X=merged_train.drop(columns=['age'])\n",
    "y=merged_train.age\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=False)\n",
    "kf.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    age_model = lgbm.LGBMRegressor(**age_params)\n",
    "    age_model.fit(X_train,y_train)\n",
    "    age_predict = age_model.predict(X_test)\n",
    "    \n",
    "age_model.fit(X,y)\n",
    "#predict age\n",
    "predicted_age = age_model.predict(age_test.drop(columns=['age']))\n",
    "merged_train.loc[age_test.index,'age'] = np.around(predicted_age, decimals=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(concat_train):\n",
    "    final_auc_uplift_score=[]\n",
    "    final_uplift_score=[]\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    skf.get_n_splits(concat_train, concat_train.target)\n",
    "    \n",
    "    for indices_learn, indices_valid in skf.split(concat_train, concat_train.target):\n",
    "        params_t = {'learning_rate':0.01,'max_depth':6,'num_leaves':10,\n",
    "                     'min_data_in_leaf':10, 'application':'binary',\n",
    "                     'subsample':0.75, 'colsample_bytree': 0.8,\n",
    "                     'reg_alpha':0.5,'data_random_seed':42,'metric':'binary_logloss',\n",
    "                     'max_bin':450,'bagging_freq':2,'reg_lambda':0.5         \n",
    "            }\n",
    "        #fit\n",
    "        transformation_model1 = ClassTransformation(AdaBoostClassifier(n_estimators=30,base_estimator=RandomForestClassifier(max_depth=1)))\n",
    "        transformation_model1.fit(\n",
    "            concat_train.iloc[indices_learn,:].drop(columns=['treatment_flg','target']),\n",
    "            concat_train.iloc[indices_learn,:]['treatment_flg'].values,\n",
    "            concat_train.iloc[indices_learn,:].target)\n",
    "        transformation_model2 = ClassTransformation(lgbm.LGBMClassifier(**params_t))\n",
    "        transformation_model2.fit(\n",
    "            concat_train.iloc[indices_learn,:].drop(columns=['treatment_flg','target']),\n",
    "            concat_train.iloc[indices_learn,:]['treatment_flg'].values,\n",
    "            concat_train.iloc[indices_learn,:].target)\n",
    "        \n",
    "        #valid\n",
    "        X_valid = concat_train.iloc[indices_valid, :].drop(columns=['treatment_flg','target'])\n",
    "        #predict\n",
    "        predict_valid = (transformation_model1.predict(X_valid)+transformation_model2.predict(X_valid))/2\n",
    "        \n",
    "        print('AUC uplift score:', round(auс_uplift(predict_valid,indices_valid,concat_train)/10**7,3))\n",
    "        print('Right uplift score:', round(get_score(predict_valid,indices_valid,concat_train),4),'\\n')\n",
    "        \n",
    "        final_auc_uplift_score.append(auс_uplift(predict_valid,indices_valid,concat_train))\n",
    "        final_uplift_score.append(get_score(predict_valid,indices_valid,concat_train))  \n",
    "\n",
    "    print('final auc uplift score =',round(sum(final_auc_uplift_score)/len(final_auc_uplift_score)/10**7,3))\n",
    "    print('final uplift score =',sum(final_uplift_score)/len(final_uplift_score))\n",
    "    \n",
    "    return round(sum(final_auc_uplift_score)/len(final_auc_uplift_score)/10**7,3),sum(final_uplift_score)/len(final_uplift_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC uplift score: 3.757\n",
      "Right uplift score: 0.0792 \n",
      "\n",
      "AUC uplift score: 3.886\n",
      "Right uplift score: 0.0685 \n",
      "\n",
      "AUC uplift score: 4.646\n",
      "Right uplift score: 0.0935 \n",
      "\n",
      "AUC uplift score: 4.761\n",
      "Right uplift score: 0.0856 \n",
      "\n",
      "AUC uplift score: 4.424\n",
      "Right uplift score: 0.0905 \n",
      "\n",
      "final auc uplift score = 4.295\n",
      "final uplift score = 0.08346419733719027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.295, 0.08346419733719027)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid(merged_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(concat_train):\n",
    "    x_names = list(concat_train.iloc[:,2:].columns)\n",
    "    diff = []\n",
    "    \n",
    "    for rs in range(1,2,1):\n",
    "        params_t = {'learning_rate':0.01,'max_depth':6,'num_leaves':10,\n",
    "                     'min_data_in_leaf':10, 'application':'binary',\n",
    "                     'subsample':0.75, 'colsample_bytree': 0.8,\n",
    "                     'reg_alpha':0.5,'data_random_seed':12,'metric':'binary_logloss',\n",
    "                     'max_bin':450,'bagging_freq':2,'reg_lambda':0.5         \n",
    "            }\n",
    "        #fit       \n",
    "        transformation_model1 = ClassTransformation(AdaBoostClassifier(n_estimators=30,base_estimator=RandomForestClassifier(max_depth=1)))\n",
    "        transformation_model1.fit(\n",
    "            concat_train.drop(columns=['treatment_flg','target']),\n",
    "            concat_train['treatment_flg'].values,\n",
    "            concat_train.target)\n",
    "        transformation_model2 = ClassTransformation(lgbm.LGBMClassifier(**params_t))\n",
    "        transformation_model2.fit(\n",
    "            concat_train.drop(columns=['treatment_flg','target']),\n",
    "            concat_train['treatment_flg'].values,\n",
    "            concat_train.target)\n",
    "\n",
    "        #test features\n",
    "        df_test['target'] = 1\n",
    "        merged_test = pd.concat([df_test,df_clients,features],axis = 1,sort = True)\n",
    "        merged_test = merged_test[~merged_test['target'].isnull()].copy()\n",
    "        merged_test['diff_quantity'] = merged_test['total_mean_product_quantity']-merged_test['two_weeks_mean_product_quantity']\n",
    "        merged_test['prop_alc'] = merged_test['total_sum_is_alcohol']/merged_test['total_sum_product_id']\n",
    "        merged_test['diff_total_points'] = merged_test['regular_points_received_all']-merged_test['regular_points_spent_all']\n",
    "        merged_test['diff_two_weeks_points'] = merged_test['regular_points_received_two_weeks']-\\\n",
    "                                                    merged_test['regular_points_spent_two_weeks']\n",
    "        merged_test['first_issue_date'] = merged_test['first_issue_date'].astype(int)/10**9\n",
    "        merged_test['first_redeem_date'] = merged_test['first_redeem_date'].astype(int)/10**9\n",
    "        merged_test['diff_time'] = merged_test['first_redeem_date']-merged_test['first_issue_date']\n",
    "        merged_test['gender'] = list(ord(v[0]) for v in merged_test['gender'].values)\n",
    "        merged_test = merged_test.fillna(0)\n",
    "        for col in ['total_mean_trans_hour','two_weeks_mean_trans_hour','total_mean_dayofweek','two_weeks_mean_dayofweek']:\n",
    "            merged_test[col] = round(merged_test.total_mean_dayofweek).astype('int')\n",
    "            merged_test[col] = merged_test[col].astype('category')\n",
    "            \n",
    "        test_x = merged_test[x_names].fillna(0)\n",
    "        predict_test = (transformation_model1.predict(test_x)+transformation_model2.predict(test_x))/2\n",
    "        diff.append(np.array(predict_test))\n",
    "    return sum(diff)/len(diff),test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit\n",
    "uplift_prediction, test_x = predict(merged_train.fillna(0))\n",
    "df_submission = pd.DataFrame({'client_id':test_x.index.values,'uplift': uplift_prediction})\n",
    "df_submission.to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
